
from keras import applications
from keras.preprocessing.image import ImageDataGenerator
from keras import optimizers
from keras.models import Sequential
from keras.layers import Dropout, Flatten, Dense
from keras import callbacks
from keras.utils import multi_gpu_model
from keras.callbacks import ModelCheckpoint  

# path to the model weights files.
# dimensions of our images.
img_width, img_height = 64, 64


train_data_dir = '/home/sm/data/DATASETS/WheatConf/train'
validation_data_dir = '/home/sm/data/DATASETS/WheatConf/val'
test_data_dir = '/home/sm/data/DATASETS/WheatConf/test'

epochs = 20
batch_size = 16

# build the VGG16 network
model = applications.VGG16(weights='imagenet', include_top=False, input_shape=(224,224,3))
print('Model loaded.')

# build a classifier model to put on top of the convolutional model
top_model = Sequential()
top_model.add(model)
         
top_model.add(Flatten())
top_model.add(Dense(256, activation='relu'))
top_model.add(Dropout(0.5))
top_model.add(Dense(40, activation='sigmoid'))

# note that it is necessary to start with a fully-trained
# classifier, including the top classifier,
# in order to successfully do fine-tuning
#top_model.load_weights(top_model_weights_path)

# add the model on top of the convolutional base


# set the first 25 layers (up to the last conv block)
# to non-trainable (weights will not be updated)
for layer in top_model.layers[:25]:
    layer.trainable = True

top_model = multi_gpu_model(top_model, gpus=8)   

#opt = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)
    
#opt = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)
 
opt = optimizers.SGD(lr=0.0001, momentum=0.9)
      
    

# compile the model with a SGD/momentum optimizer
# and a very slow learning rate.
top_model.compile(loss='categorical_crossentropy',
              optimizer=opt,
              metrics=['accuracy'])

# prepare data augmentation configuration
train_datagen = ImageDataGenerator(
    rescale=1. / 255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True)

test_datagen = ImageDataGenerator(rescale=1. / 255)

train_generator = train_datagen.flow_from_directory(
    train_data_dir,
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='categorical')

validation_generator = test_datagen.flow_from_directory(
    validation_data_dir,
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='categorical')

test_generator = test_datagen.flow_from_directory(
    test_data_dir,
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='categorical')

nb_train_samples = train_generator.n 
nb_validation_samples = validation_generator.n 


checkpoint = ModelCheckpoint(filepath='vgg16_model.h5',period=10)



# fine-tune the model
top_model.fit_generator(
    train_generator,
    samples_per_epoch=nb_train_samples,
    epochs=epochs,
    validation_data=validation_generator,
    nb_val_samples=nb_validation_samples,
    callbacks=[checkpoint]
    )


score_eva = model.evaluate_generator(test_generator,1600) #1600 testing images
print("Accuracy = ", score_eva[1])

score_predicted = model.predict_generator(test_generator,1600) #1600 testing images
print("Accuracy = ", score_predicted[1])